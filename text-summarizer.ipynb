{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0efdfb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been working on a Python script for text summarization as part of #100DaysOfCode! It uses NLP techniques, such as tokenization, stop word removal, and word frequency analysis, to generate concise summaries. üí™üî• #Python #NLP #TextSummarization\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def text_summarizer(text, num_sentences=3):\n",
    "    # Text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Text into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_words = [word for word in words if word.casefold() not in stop_words]\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    fdist = FreqDist(filtered_words)\n",
    "    \n",
    "    # Assign scores to sentences based on word frequencies\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in fdist:\n",
    "                if i in sentence_scores:\n",
    "                    sentence_scores[i] += fdist[word]\n",
    "                else:\n",
    "                    sentence_scores[i] = fdist[word]\n",
    "    \n",
    "    # Sort sentences by scores in descending order\n",
    "    sorted_sentences = sorted(sentence_scores, key=lambda x: sentence_scores[x], reverse=True)\n",
    "    \n",
    "    # Select the top `num_sentences` sentences for the summary\n",
    "    summary_sentences = sorted(sorted_sentences[:num_sentences])\n",
    "    \n",
    "    # Create the summary\n",
    "    summary = ' '.join([sentences[i] for i in summary_sentences])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Exciting news! I've been working on a Python script for text summarization as part of #100DaysOfCode! üêçüìù\n",
    "\n",
    "With this script, you can quickly summarize lengthy texts while preserving the main points. It uses NLP techniques, such as tokenization, stop word removal, and word frequency analysis, to generate concise summaries. üìö‚úÇÔ∏è\n",
    "\n",
    "Check out my GitHub repository to see the code in action and start creating your own text summarizer! Let's level up our NLP skills together! üí™üî• #Python #NLP #TextSummarization\n",
    "\"\"\"\n",
    "\n",
    "summary = text_summarizer(text)\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
